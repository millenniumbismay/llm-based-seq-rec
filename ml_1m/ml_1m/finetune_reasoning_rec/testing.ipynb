{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import os\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "device_map = 'auto'\n",
    "max_memory_mapping = {0: \"23GiB\", 1: \"23GiB\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model,                                          \n",
    "                                           token = os.environ.get(\"HUGGINGFACE_ACCESS_TOKEN\"),\n",
    ")\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token_id=32000\n"
     ]
    }
   ],
   "source": [
    "print(f\"pad_token_id={tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c389e4332d6c41e08bec2dccb6fb8d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32008, 4096)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "        max_memory = max_memory_mapping,\n",
    "        token = os.environ.get(\"HUGGINGFACE_ACCESS_TOKEN\"),\n",
    "    ).eval()\n",
    "model.resize_token_embeddings(len(tokenizer),pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"Hey, are you conscious? Can you talk to me?\",\n",
    "          \"Once upon a time there was a ghost \",\n",
    "          \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n### Instruction:\\nGiven the user preference and unpreference, identify whether the user will like the target movie by answering \"Yes.\" or \"No.\".\\n\\n### Input:\\n\\nUser Preference: \"Twister (1996)\", \"Con Air (1997)\", \"River Wild, The (1994)\", \"13th Warrior, The (1999)\", \"Batman Returns (1992)\", \"From Dusk Till Dawn (1996)\", \"Young Guns II (1990)\", \"Demolition Man (1993)\", \"Mummy, The (1999)\", \"Eraser (1996)\", \"Substitute, The (1996)\", \"Maximum Risk (1996)\", \"Sudden Death (1995)\", \"Outbreak (1995)\", \"Dick Tracy (1990)\",\\nUser Unpreference:\"Cliffhanger (1993)\",\"Armageddon (1998)\",\"Mercury Rising (1998)\",\"U.S. Marshalls (1998)\",\"Shadow, The (1994)\",\\n Whether the user will like the target movie \"Rocketeer, The (1991)\\\\?\\n\\n### Response:\\nNo.</s>\"\"\"\n",
    "          ]\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 18637, 29892,  ..., 32000, 32000, 32000],\n",
       "        [    1,  9038,  2501,  ..., 32000, 32000, 32000],\n",
       "        [    1, 13866,   338,  ...,  3782, 29889,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/m/mbismay/miniconda3/envs/mixtral/lib/python3.9/site-packages/transformers/generation/utils.py:1141: UserWarning: Input length of input_ids is 340, but `max_length` is set to 32. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/home/grads/m/mbismay/miniconda3/envs/mixtral/lib/python3.9/site-packages/transformers/generation/utils.py:1413: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 18637, 29892,  ..., 32000, 32000,  4345],\n",
      "        [    1,  9038,  2501,  ..., 32000, 32000,  4345],\n",
      "        [    1, 13866,   338,  ..., 29889,     2,    13]])\n"
     ]
    }
   ],
   "source": [
    "generate_ids = model.generate(inputs.input_ids, max_length=32)\n",
    "print(generate_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey, are you conscious? Can you talk to me?MS',\n",
       " 'Once upon a time there was a ghost MS',\n",
       " 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n### Instruction:\\nGiven the user preference and unpreference, identify whether the user will like the target movie by answering \"Yes.\" or \"No.\".\\n\\n### Input:\\n\\nUser Preference: \"Twister (1996)\", \"Con Air (1997)\", \"River Wild, The (1994)\", \"13th Warrior, The (1999)\", \"Batman Returns (1992)\", \"From Dusk Till Dawn (1996)\", \"Young Guns II (1990)\", \"Demolition Man (1993)\", \"Mummy, The (1999)\", \"Eraser (1996)\", \"Substitute, The (1996)\", \"Maximum Risk (1996)\", \"Sudden Death (1995)\", \"Outbreak (1995)\", \"Dick Tracy (1990)\",\\nUser Unpreference:\"Cliffhanger (1993)\",\"Armageddon (1998)\",\"Mercury Rising (1998)\",\"U.S. Marshalls (1998)\",\"Shadow, The (1994)\",\\n Whether the user will like the target movie \"Rocketeer, The (1991)\\\\?\\n\\n### Response:\\nNo.\\n']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([2], skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 2, 0, 3, 0, 3, 4, 45, 0, 3], [0, 2, 0, 3, 0, 3, 4, 45, 0, 3]],\n",
       " [[1, 2, 1, 3, 1, 3, 4, 45, 1, 3], [1, 2, 1, 3, 1, 3, 4, 45, 1, 3]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [[1,2,1,3,1,3,4,45,1,3], [1,2,1,3,1,3,4,45,1,3]]\n",
    "l1 = [[n if n!=1 else 0 for n in label] for label in l]\n",
    "l1, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mixtral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
